# -*- coding: utf-8 -*-
"""Alzheimer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l4kkbSoPKhB9uuySUvnQkLlDus-r53dB

# Proyecto: Predicción de Alzheimer a partir de MRI usando Deep Learning

## **0. Integrantes del equipo de trabajo**
---
1. RAÚL RAMÍREZ PENAGOS
2. IVÁN QUEVEDO
3. LINA ÁVILA MORENO

## **1. Entendimiento de los datos**

El conjunto de datos contiene 4 clases y etiquetas del 0 al 3:

1. Non demented : '0'
2. Very midly demented : '1'
3. Mildly demented : '2'
4. Moderately demented : '3'
"""

import pandas as pd
import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import hashlib
import matplotlib.cm as cm
import seaborn as sns
from sklearn.decomposition import PCA

from google.colab import drive
drive.mount('/content/drive')

df_train = pd.read_parquet('/content/drive/MyDrive/DIPLOMADO2/PROYECTO3_MLDS_AVANZADO/train.parquet')
df_test = pd.read_parquet('/content/drive/MyDrive/DIPLOMADO2/PROYECTO3_MLDS_AVANZADO/test.parquet')

df_train.head()

df_test.head()

df_test.info()

df_train.info()

#Creación de la carpeta de destino solicitada en data_definition.md (entregable 2)
dest_dir = "/content/drive/MyDrive/DIPLOMADO2/PROYECTO3_MLDS_AVANZADO/procesado/"
os.makedirs(dest_dir, exist_ok=True)

etiqueta_enfermedad_categoria = {
    0: "Mild Demented",
    1: "Moderate Demented",
    2: "Non Demented",
    3: "Very Mild Demented",
}

def decodificar_imagen(image_dict):
    if isinstance(image_dict, dict) and "bytes" in image_dict:
        byte_string = image_dict["bytes"]
        datos_raw_np = np.frombuffer(byte_string, np.uint8)

        # Cargar en escala de grises (1 canal)
        img = cv2.imdecode(datos_raw_np, cv2.IMREAD_GRAYSCALE)

        # Redimensionar
        img = cv2.resize(img, (224, 224))

        # Expandir a 3 canales se replicó la imagen en gris
        img_3ch = np.stack([img, img, img], axis=-1)

        return img_3ch

    else:
        raise TypeError(f"Se esperaba un diccionario {type(image_dict)}")

#Verifico que tenga los 3 canales
img_dict = df_train["image"].iloc[0]
img_3ch = decodificar_imagen(img_dict)
print(img_3ch.shape)

fig, ax = plt.subplots(2, 2, figsize=(10, 7))
axs = ax.flatten()
for axes in axs:
    rand = np.random.randint(0, len(df_train))
    decoded_image = decodificar_imagen(df_train.iloc[rand]['image'])
    axes.imshow(decoded_image, cmap="gray")
    axes.set_title(etiqueta_enfermedad_categoria[df_train.iloc[rand]['label']])
plt.tight_layout()
plt.show()

#Valores Faltantes
print(df_train.isnull().sum())

#Valores Faltantes
print(df_test.isnull().sum())

"""## **2. Limpieza de los datos**"""

#Imágenes corruptas

df_train['img_arr'] = df_train['image'].apply(decodificar_imagen)
df_test['img_arr'] = df_test['image'].apply(decodificar_imagen)


def imagen_valida(img_array):
    return img_array is not None and img_array.size > 0

# Aplicar validación de imágenes corruptas en df_train
init_train = len(df_train)
df_train = df_train[df_train["img_arr"].apply(imagen_valida)]
cleaned_train = len(df_train)
print(f"Filas de entrenamiento antes de limpiar: {init_train}")
print(f"Filas de entrenamiento después de limpiar: {cleaned_train}")
print(f"Filas de entrenamiento eliminadas: {init_train - cleaned_train}")

# Aplicar validación de imágenes corruptas en df_test
init_test = len(df_test)
df_test = df_test[df_test["img_arr"].apply(imagen_valida)]
cleaned_test = len(df_test)
print(f"Filas de prueba antes de limpiar: {init_test}")
print(f"Filas de prueba después de limpiar: {cleaned_test}")
print(f"Filas de prueba eliminadas: {init_test - cleaned_test}")

#Duplicados

def hash_image(img):
    return hashlib.md5(img.tobytes()).hexdigest()

df_train["hash"] = df_train["img_arr"].apply(hash_image)
df_test["hash"] = df_test["img_arr"].apply(hash_image)

duplicados = df_train.duplicated(subset="hash").sum()
print("Número de imágenes duplicadas en train:", duplicados)

duplicados_test = df_test.duplicated(subset="hash").sum()
print("Número de imágenes duplicadas en test:", duplicados_test)

df_train.info()

#Elimino la columna hash creada previamente
df_train = df_train.drop(columns="hash")
df_test = df_test.drop(columns="hash")

#df_train.info()

#Balanceo de clases
df_train["label"].value_counts()

plt.figure(figsize=(9, 5))
num_classes = df_train['label'].nunique()
colors = cm.get_cmap('viridis', num_classes)
plt.bar(np.arange(0, 4, 1), df_train['label'].value_counts().sort_index(), color=[colors(i) for i in range(num_classes)])
plt.ylabel("Número de imágenes")
plt.xticks(np.arange(0, 4, 1), labels=[etiqueta_enfermedad_categoria[i] for i in range(4)])
plt.show()
print(f"Total muestras en train = {len(df_train)}")

df_train.head()

#Normalizar

# 1. Normalización de train
# Antes de normalizar
print("Antes de normalizar:")
print(df_train['img_arr'].iloc[0])
print("Valores mínimo y máximo:", df_train['img_arr'].iloc[0].min(), df_train['img_arr'].iloc[0].max())

# Aplicando normalización [0,1]
df_train['img_arr'] = df_train['img_arr'].apply(lambda x: x / 255.0)

# Después de normalizar
print("\nDespués de normalizar:")
print(df_train['img_arr'].iloc[0])
print("Valores mínimo y máximo:", df_train['img_arr'].iloc[0].min(), df_train['img_arr'].iloc[0].max())

# 1. Normalización de test
# Antes de normalizar
print("Antes de normalizar:")
print(df_test['img_arr'].iloc[0])
print("Valores mínimo y máximo:", df_test['img_arr'].iloc[0].min(), df_test['img_arr'].iloc[0].max())

# Aplicando normalización [0,1]
df_test['img_arr'] = df_test['img_arr'].apply(lambda x: x / 255.0)

# Después de normalizar
print("\nDespués de normalizar:")
print(df_test['img_arr'].iloc[0])
print("Valores mínimo y máximo:", df_test['img_arr'].iloc[0].min(), df_test['img_arr'].iloc[0].max())

"""## **3. Relaciones entre variables**"""

#Imágenes por clase
fig, axes = plt.subplots(1, 4, figsize=(12, 3))
classes = [0, 1, 2, 3]

for i, c in enumerate(classes):
    img = df_train[df_train.label==c]["img_arr"].values[0]
    ax = axes[i]
    ax.imshow(img[:,:,0], cmap='gray')
    ax.set_title(etiqueta_enfermedad_categoria[c])
    ax.axis('off')

plt.tight_layout()
plt.show()

#Resumen estadístico por clase
df_train['mean_pixel'] = df_train['img_arr'].apply(lambda x: x.mean())
df_train['std_pixel']  = df_train['img_arr'].apply(lambda x: x.std())
sns.boxplot(x='label', y='mean_pixel', data=df_train)

df_train.groupby('label')['mean_pixel'].describe()

#PCA